{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Reviews Analysis\n",
    "Data source : https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "EDA: https://nycdatascience.com/blog/student-works/amazon-fine-foods-visualization/\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n",
    "\n",
    "Number of reviews: 568,454 <br>\n",
    "Number of users: 256,059 <br>\n",
    "Number of products: 74,258 <br>\n",
    "Timespan: Oct 1999 - Oct 2012 <br>\n",
    "Number of Attributes/Columns in data: 10  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Classfication of positive and negative reviews used ML algo\n",
    "Given a review, determine wheter the review is positive (rating 4 or 5) or negative\n",
    "\n",
    "Given a review, determine whether the review is positive (rating of 4 or 5) or negative (rating of 1 or 2).\n",
    "\n",
    "__[Q]__ How to determine if a review is positive or negative?\n",
    "\n",
    "__[Ans]__ We could use Score/Rating. A rating of 4 or 5 can be cosnidered as a positive review. A rating of 1 or 2 can be considered as negative one. A review of rating 3 is considered nuetral and such reviews are ignored from our analysis. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#for large and multi-dim arrays\n",
    "import numpy as np\n",
    "#for data manipulation and analysis\n",
    "import pandas as pd\n",
    "#Natural language processing\n",
    "import nltk\n",
    "#Stopwords corpus\n",
    "from nltk.corpus import stopwords\n",
    "#Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "#Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#TF_IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see the Score column, it has values 1,2,3,4. Considering 1,2 as Negative and 4,5 as positive, the Neutral is 3, we will delete the rows that neutral\n",
    "\n",
    "HelfulnessNumerator says about number of people found review usefull and HelpfulnessDenomitor usefull review count + not so usefull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove neutral review\n",
    "df_1 = df[df['Score'] != 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the score to categorical, Positive and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(score):\n",
    "    if score > 3:\n",
    "        return 'Positive'\n",
    "    return 'Negative'\n",
    "\n",
    "score = df_1['Score']\n",
    "convert = score.map(partition)\n",
    "df_1['Score'] = convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  Positive  1303862400   \n",
       "1                     0                       0  Negative  1346976000   \n",
       "2                     1                       1  Positive  1219017600   \n",
       "3                     3                       3  Negative  1307923200   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORATPRY DATA ANALYSIS\n",
    "that will help us to undrestand our data\n",
    "\n",
    "#### Preprocessig\n",
    "1st: We can't work with the text data in ML, so we need to convert them in to numerical vectors\n",
    "\n",
    "2nd: Text data needs to cleaned and encoded into numerical values before given then, this cleaning and encoding is called as Text Preporcessing\n",
    "\n",
    "Understanding the data- what's data is all about, what should be considered for cleaning for data (punctuations, stopwords, ect...)\n",
    "\n",
    "Some Encoding Techniques:\n",
    "\n",
    " - Bag of words\n",
    " - Binary bag of words\n",
    " - Bigram, Ngram\n",
    " - TF, IDF\n",
    " - Word2Vec\n",
    " - TF-IDF Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting data for time based splitting for model train and test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df_1[\"Time\"] = df_1[\"Time\"].map(lambda t: datetime.datetime.fromtimestamp(int(t)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "sortedData = df_1.sort_values('ProductId',axis=0,kind=\"quicksort\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Reviews may have duplicate entriees. Hence it is necessry to remove diplicates in order to get unbiased results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "final = sortedData.drop_duplicates(subset={'UserId','ProfileName','Time','Text'}, keep=\"first\", inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Helfluness numerator should always be less thank Denominator\n",
    "           - Helpfulness Numerator : number of people found that review are useful\n",
    "           - Hulpfulness Denominator : is about useful review count + not so useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[final['HelpfulnessNumerator'] <= final['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As data is huge, we are limiting our data due to computation limition, we will try to pick data in a way so that it does't make data imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19313, 10)\n"
     ]
    }
   ],
   "source": [
    "# As data is huge, due to computation limitation we will randomly select data. we will try to pick data in a way so that it doesn't make data imbalance problem\n",
    "finalp = final[final.Score == 'Positive']\n",
    "finalp = finalp.sample(frac=0.035,random_state=1) #0.055\n",
    "\n",
    "finaln = final[final.Score == 'Negative']\n",
    "finaln = finaln.sample(frac=0.15,random_state=1) #0.25\n",
    "\n",
    "final = pd.concat([finalp,finaln],axis=0)\n",
    "\n",
    "#sording data by timestamp so that it can be devided in train and test dataset for time based slicing.\n",
    "final = final.sort_values('Time',axis=0,kind=\"quicksort\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all words to lowercase and removing punctuations and html tags if any\n",
    "\n",
    "__Stemming__ : Converting the words into their base word or stem word (Ex- tastfully, tasty these words converted to stem word called 'tasti'). This reduces the vector dimension\n",
    "\n",
    "__Stopwords__ : are unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aren', 'being', 'own', 'about', \"she's\", 'i', 'that', 'we', 'both', 'same', \"you'd\", 'he', 'after', 'm', 'these', \"don't\", 'from', 'am', 'each', 'but', 'can', 'should', 'now', 'ain', \"weren't\", 'shouldn', \"you're\", 'did', 'why', 'o', 'd', 'mightn', 'himself', 'my', 'weren', 'those', 'don', 'so', 'been', \"you've\", \"shouldn't\", 'above', 'her', 'a', 'in', 'below', 'too', 'yourselves', 'if', 'will', 'myself', 'until', 't', 're', \"that'll\", 'all', 'll', 'before', \"aren't\", 'hers', 'over', 'ourselves', 'ours', 'by', 'haven', \"hasn't\", 'yourself', 'once', 'an', 'do', 's', 'up', 'hasn', 'has', 'further', 'what', 'such', 'shan', \"wouldn't\", 'and', 'most', 'our', \"shan't\", 'with', 'other', 'who', 'no', 'ma', 've', 'as', 'does', 'the', 'isn', 'him', 'me', 'than', 'on', 'when', 'only', 'then', \"wasn't\", 'wouldn', 'doing', 'more', 'mustn', 'against', 'whom', 'itself', 'wasn', \"you'll\", 'just', 'they', 'yours', 'very', 'nor', \"should've\", 'here', 'are', 'it', 'is', \"mightn't\", 'few', 'again', 'your', \"won't\", 'have', 'to', 'into', 'how', 'during', 'won', 'its', 'some', \"haven't\", 'were', 'where', 'couldn', 'this', 'you', 'needn', 'had', 'was', \"isn't\", 'any', 'between', 'out', 'y', \"it's\", 'themselves', 'she', 'them', 'theirs', 'at', \"couldn't\", \"needn't\", 'their', 'didn', 'having', 'for', 'not', 'herself', 'under', \"mustn't\", 'be', 'doesn', 'which', 'of', 'while', 'down', \"didn't\", \"hadn't\", \"doesn't\", 'his', 'or', 'hadn', 'through', 'there', 'because', 'off'}\n"
     ]
    }
   ],
   "source": [
    "# all english stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = set(stopwords.words('english')) # set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') # initialisig the snowball stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove html-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delHtml(sentence):\n",
    "    cleaner = re.compile('<.*?>')\n",
    "    cleanedtext = re.sub(cleaner,' ', sentence)\n",
    "    return cleanedtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove punctuation or special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delpun_sp(sentence):\n",
    "    cleaner_punc = re.compile('[?|!|\\'|\"|#]')\n",
    "    cleaned = re.sub(cleaner_punc,'', sentence)\n",
    "    cleaner_spe = re.compile('[.|,|)|(|\\|/]')\n",
    "    cleand = re.sub(cleaner_spe, ' ', cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "str1 = 1\n",
    "final_X =[]\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "s=''\n",
    "\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=delHtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in delpun_sp(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words    \n",
    "    final_X.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG OF WORD (BOW)\n",
    "\n",
    "In BoW we construct a dictionary that contains set of all unique words from our test review dataset. The frequency of the word is counted here, if there are d unique words in our dictionary then for every sentence or review the vector will be of lenght d and count of word from review is stored at its particular location in vector. The vector will be highly sparce in such case.\n",
    "\n",
    "Ex: pasta is tasty and pasta is good [0].....[1]....[1]....[2]....[2]....[1].... <== its vector representation [a]...[and]..[is]..[pasta]...[tasty] <== This is dictonary\n",
    "\n",
    "Using sckit-learn's __CountVectorize__ we can get the BoW and check out all the parameters it consists of, one of them is __max_features=5000__ it tells about to consider only top 5000 most frequently repeated words to place in a dictionary. so our dictionary lenght or vector lenght will be only 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BINARY BAG OF WORDS\n",
    "In binary BoW, we dont count the frequency of word, we just place 1 if word appears in the review or else 0. In __CountVectorizer__ there is a parameter __binary=True__ this makes our Bow to binary BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "final_bow_count = count_vect.fit_transform(final_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drawbacks of BoW/ Binary BoW__\n",
    "Our main objective in doing these text to vector encodings, is that similar meaning text vectors should be close to each other, but in some cases this my not possible for BoW\n",
    "\n",
    "For example, if we consider two reviews, **This pasta is very testy** and **this pasta is not tasty**, after stopwords removel both sentences will be converted to **pasta tasty** so both giving exact same meaning.\n",
    "\n",
    "The main problem is here we are not considering the front and back words related to every word, here comes __Bigram__ and __Ngram__ techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BI-GRAM BOW__\n",
    "Considering pair of words for creating dictionary is Bi-Gram, Tri-Gram means three consecutive words so as NGram\n",
    "\n",
    "__CountVectorizer__ has a parameter __ngram_range__ if assigned to (1,2) it considers Bi-Gram BoW\n",
    "\n",
    "But this massively increase our dictonary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data, Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with standard deviation of 1\n",
    "\n",
    "To learn more about it, http://benalexkeen.com/feature-scaling-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "final_bow_np = StandardScaler(with_mean=False).fit_transform(final_bow_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "We already have sorted data by timestamp, so we will use first 70% as Train set with cross validation and next 30% for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_bow_np\n",
    "y = final['Score']\n",
    "\n",
    "X_train = final_bow_np[:math.ceil(len(final)*.7)]\n",
    "X_test = final_bow_np[math.ceil(len(final)*0.7):]\n",
    "\n",
    "y_train = y[:math.ceil(len(final)*.7)]\n",
    "y_test = y[math.ceil(len(final)*.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
